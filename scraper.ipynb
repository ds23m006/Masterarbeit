{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pymongo selenium dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cmd\n",
    "#playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "@register_cell_magic\n",
    "def deactivate(line, cell):\n",
    "    \"\"\"\n",
    "    Magic cell Funktion: Deaktiviert die Ausführung der Zelle.\n",
    "    \"\"\"\n",
    "    message = \"⚠️ **Diese Zelle ist deaktiviert und wurde nicht ausgeführt.**\"\n",
    "    display(Markdown(message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "import socket\n",
    "\n",
    "CHROMEDRIVER_PATH = \"/usr/bin/chromedriver\" if socket.gethostname() == \"raspberrypi\" else \"chromedriver.exe\"\n",
    "FRONTPAGE_URL = \"https://www.derstandard.at/\"\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_mongodb.py\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "\n",
    "# set user creds\n",
    "username = os.getenv(\"MONGODB_USER\")\n",
    "password = os.getenv(\"MONGODB_PWD\")\n",
    "\n",
    "# setup client\n",
    "client = MongoClient(f\"mongodb://{username}:{password}@BlackWidow:27017\")\n",
    "\n",
    "# load db and collections\n",
    "db = client['newspapers']\n",
    "derStandard_collection = db['derStandard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger_setup.py\n",
    "\n",
    "import logging\n",
    "\n",
    "def setup_logger(name=__name__, log_file='scraper.log', level=logging.DEBUG):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Log-Format definieren\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Console-Handler hinzufügen\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # File-Handler hinzufügen\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "\n",
    "def expand_shadow_element(driver, element: WebElement):\n",
    "    \"\"\"Erweitert ein Shadow DOM-Element und gibt das Shadow Root zurück.\"\"\"\n",
    "    shadow_root = driver.execute_script('return arguments[0].shadowRoot', element)\n",
    "    return shadow_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.py\n",
    "\n",
    "import time\n",
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import datetime\n",
    "\n",
    "def configure_driver(headless=True):\n",
    "    chrome_options = wd.ChromeOptions()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(f\"--user-agent={USER_AGENT}\")\n",
    "    chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    chrome_prefs = {\n",
    "        \"profile.default_content_settings.images\": 2,\n",
    "        \"profile.managed_default_content_settings.images\": 2\n",
    "    }\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_options.page_load_strategy = 'none'\n",
    "    \n",
    "    service = ChromeService(executable_path=CHROMEDRIVER_PATH)\n",
    "    driver = wd.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    # POPUP WEGKLICKEN\n",
    "    driver.get(FRONTPAGE_URL + datetime.date.today().strftime(\"%Y/%m/%d\"))\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            lambda d: d.execute_script(\"return document.readyState\") == 'complete'\n",
    "        )\n",
    "        driver.switch_to.frame(driver.find_element(By.XPATH, \"/html/body/div/iframe\"))\n",
    "        driver.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div[3]/div[1]/button\").click()\n",
    "        driver.switch_to.parent_frame()\n",
    "    except NoSuchElementException:\n",
    "        raise Exception(\"Popup nicht gefunden\")\n",
    "    \n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsers.py\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "import dateparser\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import inspect\n",
    "\n",
    "def parse_posting(posting_element, logger):\n",
    "    # Parst ein einzelnes <dst-posting>-Element und extrahiert die relevanten Daten.\n",
    "    try:\n",
    "        # Extrahiere den Autor\n",
    "        author = \"Unbekannter Benutzer\"\n",
    "        try:\n",
    "            usermenu = posting_element.find_element(By.CSS_SELECTOR, \"dst-posting--user button\")\n",
    "            spans = usermenu.find_elements(By.CSS_SELECTOR, \"span > span\")\n",
    "            if spans:\n",
    "                author = spans[0].text.strip()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Extrahiere die Anzahl der Follower\n",
    "        user_followers = 0\n",
    "        try:\n",
    "            followers_div = posting_element.find_element(By.CSS_SELECTOR, \"dst-posting--user button div[title]\")\n",
    "            followers_text = followers_div.get_attribute(\"title\")\n",
    "            followers_match = re.search(r'\\d+', followers_text)\n",
    "            if followers_match:\n",
    "                user_followers = int(followers_match.group())\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        except ValueError:\n",
    "            logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Ungültige Follower-Zahl: '{followers_text}'.\")\n",
    "\n",
    "        # Extrahiere das Datum und die Uhrzeit\n",
    "        datetime_obj = None\n",
    "        try:\n",
    "            time_tag = posting_element.find_element(By.CSS_SELECTOR, \"time[data-date]\")\n",
    "            datetime_str = time_tag.get_attribute(\"data-date\")\n",
    "            datetime_obj = dateparser.parse(datetime_str, languages=['de'])\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Extrahiere den Inhalt des Postings\n",
    "        content = \"\"\n",
    "        try:\n",
    "            content_div = posting_element.find_element(By.CSS_SELECTOR, \"div.posting--content\")\n",
    "            headers = content_div.find_elements(By.TAG_NAME, \"h1\")\n",
    "            paragraphs = content_div.find_elements(By.TAG_NAME, \"p\")\n",
    "            header_text = \"\\n\".join([h.text for h in headers]) if headers else \"\"\n",
    "            paragraph_text = \"\\n\".join([p.text for p in paragraphs]) if paragraphs else \"\"\n",
    "            content = \"\\n\".join([header_text, paragraph_text]).strip()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Extrahiere Upvotes und Downvotes\n",
    "        upvotes = 0\n",
    "        downvotes = 0\n",
    "        try:\n",
    "            ratinglog = posting_element.find_element(By.CSS_SELECTOR, \"dst-posting--ratinglog\")\n",
    "            positiveratings = ratinglog.get_attribute(\"positiveratings\")\n",
    "            negativeratings = ratinglog.get_attribute(\"negativeratings\")\n",
    "            upvotes = int(positiveratings) if positiveratings and positiveratings.isdigit() else 0\n",
    "            downvotes = int(negativeratings) if negativeratings and negativeratings.isdigit() else 0\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        except ValueError:\n",
    "            logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Ungültige Upvote/Downvote-Zahlen gefunden.\")\n",
    "\n",
    "        # Extrahiere Parent-Kommentar-ID (falls Antwort)\n",
    "        parent_id = posting_element.get_attribute(\"data-parentpostingid\")\n",
    "        reply_on_comment = int(parent_id) if parent_id and parent_id.isdigit() else None\n",
    "\n",
    "        # Extrahiere Kommentar-ID\n",
    "        commentID = posting_element.get_attribute(\"data-postingid\")\n",
    "        commentID = int(commentID) if commentID and commentID.isdigit() else None\n",
    "\n",
    "        # Erstelle das Kommentar-Dictionary\n",
    "        comment = {\n",
    "            'commentID': commentID,\n",
    "            'author': author,\n",
    "            'user_followers': user_followers,\n",
    "            'datetime': datetime_obj,\n",
    "            'content': content,\n",
    "            'upvotes': upvotes,\n",
    "            'downvotes': downvotes,\n",
    "            'reply_on_comment': reply_on_comment,\n",
    "            'replies': []\n",
    "        }\n",
    "\n",
    "        return comment\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{inspect.currentframe().f_back.f_code.co_name} Fehler beim Parsen eines Postings: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def parse_comment_datetime(datetime_str):\n",
    "    return dateparser.parse(datetime_str, languages=['de'])\n",
    "\n",
    "def get_article_byline(soup, logger):\n",
    "    article_byline = {}\n",
    "    article_byline_tag = soup.find('div', class_='article-byline')\n",
    "    if article_byline_tag:\n",
    "        # Storylabels extrahieren\n",
    "        storylabels_tag = article_byline_tag.find('div', class_='storylabels')\n",
    "        if storylabels_tag:\n",
    "            storylabels = storylabels_tag.get_text(strip=True)\n",
    "            article_byline['storylabels'] = storylabels\n",
    "\n",
    "        # Article origins extrahieren\n",
    "        article_origins_tag = article_byline_tag.find('div', class_='article-origins')\n",
    "        if article_origins_tag:\n",
    "            article_origins = article_origins_tag.get_text(strip=True)\n",
    "            article_byline['article_origins'] = article_origins\n",
    "        else:\n",
    "            # Fallback für einfachen Autorentext\n",
    "            author_simple = article_byline_tag.find('span', class_='simple')\n",
    "            if author_simple:\n",
    "                article_byline['article_origins'] = author_simple.get_text(strip=True)\n",
    "    else:\n",
    "        article_byline = None\n",
    "        logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Keine Artikel-Byline gefunden.\")\n",
    "    return article_byline\n",
    "\n",
    "def get_article_datetime(soup, logger):\n",
    "    time_tag = soup.find('time', class_='article-pubdate')\n",
    "    if time_tag:\n",
    "        if time_tag.has_attr('datetime'):\n",
    "            datetime_str = time_tag['datetime'].strip()\n",
    "            datetime_str = datetime_str.replace('\\n', '').strip()\n",
    "        else:\n",
    "            datetime_str = time_tag.get_text(strip=True)\n",
    "        try:\n",
    "            article_datetime = datetime.datetime.fromisoformat(datetime_str)\n",
    "        except ValueError:\n",
    "            datetime_text = time_tag.get_text(strip=True)\n",
    "            article_datetime = dateparser.parse(datetime_text, languages=['de'])\n",
    "    else:\n",
    "        article_datetime = None\n",
    "        logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Kein Datum gefunden.\")\n",
    "    return article_datetime\n",
    "\n",
    "def get_posting_count(soup, full_url, logger):\n",
    "    posting_count = None\n",
    "    try:\n",
    "        posting_count_tag = soup.find('span', class_='js-forum-postingcount')\n",
    "        if posting_count_tag:\n",
    "            posting_count_text = posting_count_tag.contents[0].strip()\n",
    "            posting_count = int(posting_count_text)\n",
    "            return posting_count\n",
    "    except (AttributeError, ValueError):\n",
    "        posting_count = None\n",
    "    try:\n",
    "        community_section = soup.find('section', id='story-community')\n",
    "        header_div = community_section.find('div', class_='story-community-header')\n",
    "        h1_tag = header_div.find('h1')\n",
    "        h1_text = h1_tag.get_text(strip=True)\n",
    "        match = re.search(r'Forum:\\s*(\\d+)\\s*Postings', h1_text)\n",
    "        posting_count = int(match.group(1))\n",
    "        return posting_count\n",
    "    except (AttributeError, ValueError):\n",
    "        posting_count = None\n",
    "        logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Ungültige Posting-Anzahl in {full_url}\")\n",
    "    return posting_count\n",
    "\n",
    "def get_paragraph_texts(soup, full_url, logger):\n",
    "    # Artikelinhalt extrahieren\n",
    "    paragraph_texts = None\n",
    "    try:\n",
    "        article_body = soup.find('div', class_='article-body')\n",
    "        if article_body:\n",
    "\n",
    "            logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Alle 'href'-Attribute aus Artikeltext entfernt.\")\n",
    "\n",
    "            # Unerwünschte Elemente entfernen\n",
    "            for ad in article_body.find_all(['ad-container', 'ad-slot', 'ad', 'native-ad']):\n",
    "                ad.decompose()\n",
    "            for ad in article_body.find_all(\"div\", class_=\"native-ad\"):\n",
    "                ad.decompose()\n",
    "            for figure in article_body.find_all('figure'):\n",
    "                figure.decompose()\n",
    "            for unwanted in article_body.find_all(['aside', 'nav', 'div'], attrs={'data-section-type': 'supplemental'}):\n",
    "                unwanted.decompose()\n",
    "\n",
    "            # Alle 'href'-Attribute entfernen\n",
    "            for a_tag in article_body.find_all('a'):\n",
    "                a_tag['href'] = ''\n",
    "\n",
    "            logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Unerwünschte Elemente aus Artikeltext entfernt.\")\n",
    "\n",
    "            # Paragraphen extrahieren und in Liste umwandeln\n",
    "            paragraphs = article_body.find_all('p')\n",
    "            paragraph_texts = [p.get_text() for p in paragraphs]\n",
    "            logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Extrahierte Paragraphen: {len(paragraph_texts)} in {full_url}\")\n",
    "        else:\n",
    "            logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Kein Artikelinhalt gefunden in {full_url}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{inspect.currentframe().f_back.f_code.co_name} Fehler beim Extrahieren des Artikelinhalts: {e}\", exc_info=True)\n",
    "    return paragraph_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper.py\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import multiprocessing\n",
    "\n",
    "PUBLISHER = \"derStandard\"\n",
    "\n",
    "def extract_reactions(driver, logger):\n",
    "    \"\"\"\n",
    "    Extrahiert die Reaktionen aus dem Shadow DOM der aktuellen Seite.\n",
    "    Gibt ein Tuple zurück: (reactions_dict, warning_flag)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shadow_host = driver.find_element(By.CSS_SELECTOR, \"dst-community-reactions\")\n",
    "        shadow_root = expand_shadow_element(driver, shadow_host)\n",
    "        reactions_buttons = shadow_root.find_elements(By.CSS_SELECTOR, \"aside.reactions div.reactions--buttons button\")\n",
    "        reactions = {}\n",
    "        for button in reactions_buttons:\n",
    "            try:\n",
    "                count_element = button.find_element(By.TAG_NAME, \"strong\")\n",
    "                count = int(count_element.text.strip())\n",
    "            except (NoSuchElementException, ValueError):\n",
    "                count = 0\n",
    "            try:\n",
    "                sr_only = button.find_element(By.CSS_SELECTOR, \"span.sr-only\")\n",
    "                reaction_name = sr_only.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                reaction_name = button.text.replace(str(count), '').strip()\n",
    "            reactions[reaction_name] = count\n",
    "        return reactions, False  # Kein Warnhinweis notwendig\n",
    "    except NoSuchElementException:\n",
    "        logger.warning(\"Reaktionen konnten nicht extrahiert werden.\")\n",
    "        return None, True  # Warnhinweis setzen\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler beim Extrahieren der Reaktionen: {e}\", exc_info=True)\n",
    "        return None, True  # Warnhinweis setzen\n",
    "    \n",
    "    \n",
    "\n",
    "def extract_forum_comments_normal(driver, logger, max_comments=70):\n",
    "    \"\"\"\n",
    "    Extrahiert Benutzerkommentare aus dem Shadow DOM der aktuellen Seite und\n",
    "    bildet verschachtelte Antworten ab.\n",
    "    Gibt ein Tuple zurück: (comments_list, warning_flag)\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    count = 0\n",
    "    try:\n",
    "        forum_host = driver.find_element(By.CSS_SELECTOR, \"dst-forum\")\n",
    "        forum_shadow = expand_shadow_element(driver, forum_host)\n",
    "        main_content = forum_shadow.find_element(By.CSS_SELECTOR, \"main.forum--main\")\n",
    "        children = main_content.find_elements(By.CSS_SELECTOR, \":scope > *\")\n",
    "        current_parent = None\n",
    "\n",
    "        for child in children:\n",
    "            if count >= max_comments:\n",
    "                break\n",
    "            tag_name = child.tag_name.lower()\n",
    "            if tag_name == \"dst-posting\":\n",
    "                comment = parse_posting(child, logger)\n",
    "                if comment:\n",
    "                    comments.append(comment)\n",
    "                    current_parent = comment\n",
    "                    count += 1\n",
    "            elif tag_name == \"section\":\n",
    "                classes = child.get_attribute(\"class\")\n",
    "                if classes and \"thread\" in classes:\n",
    "                    if not current_parent:\n",
    "                        logger.warning(\"Thread-Sektion gefunden, aber kein aktueller Parent.\")\n",
    "                        continue\n",
    "                    reply_postings = child.find_elements(By.CSS_SELECTOR, \"dst-posting\")\n",
    "                    for reply in reply_postings:\n",
    "                        if count >= max_comments:\n",
    "                            break\n",
    "                        reply_comment = parse_posting(reply, logger)\n",
    "                        if reply_comment:\n",
    "                            current_parent['replies'].append(reply_comment)\n",
    "                            count += 1\n",
    "        return comments, False  # Kein Warnhinweis notwendig\n",
    "    except NoSuchElementException:\n",
    "        logger.warning(\"Forum-Elemente nicht gefunden.\")\n",
    "        return [], True  # Warnhinweis setzen\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler beim Extrahieren der Forenkommentare: {e}\", exc_info=True)\n",
    "        return [], True  # Warnhinweis setzen\n",
    "    \n",
    "\n",
    "def extract_forum_comments_alternative(driver, logger, max_comments=70):\n",
    "    \"\"\"\n",
    "    Extrahiert Benutzerkommentare aus der aktuellen Seite unter Verwendung von BeautifulSoup\n",
    "    und bildet verschachtelte Antworten ab.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_map = {}\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    postings = soup.find_all('div', class_='posting', attrs={'data-postingid': True})\n",
    "\n",
    "    if not postings:\n",
    "        logger.warning(\"Forum-Elemente nicht gefunden.\")\n",
    "        return comments_data, True  # Warnhinweis setzen\n",
    "\n",
    "\n",
    "    for posting in postings[:max_comments]:\n",
    "        try:\n",
    "            commentID = posting.get('data-postingid')\n",
    "            if not commentID or not commentID.isdigit():\n",
    "                continue\n",
    "            commentID = int(commentID)\n",
    "\n",
    "            username = posting.get('data-communityname') or 'gelöschtes Profil'\n",
    "\n",
    "            reply_on_comment = posting.get('data-parentpostingid')\n",
    "            reply_on_comment = int(reply_on_comment) if reply_on_comment and reply_on_comment.isdigit() else None\n",
    "\n",
    "            # Datum und Uhrzeit des Kommentars extrahieren\n",
    "            datetime_tag = posting.find('span', class_='js-timestamp')\n",
    "            if datetime_tag and datetime_tag.text:\n",
    "                datetime_str = datetime_tag.text.strip()\n",
    "                datetime_obj = parse_comment_datetime(datetime_str)\n",
    "            else:\n",
    "                datetime_obj = None \n",
    "\n",
    "            # Kommentarüberschrift extrahieren\n",
    "            comment_header_tag = posting.find('h4', class_='upost-title')\n",
    "            comment_header = comment_header_tag.text.strip() if comment_header_tag else \"\"\n",
    "\n",
    "            # Kommentartext extrahieren\n",
    "            comment_body = posting.find('div', class_='upost-text')\n",
    "            comment_text = comment_body.get_text(separator=' ', strip=True) if comment_body else \"\"\n",
    "\n",
    "            # Upvotes extrahieren\n",
    "            upvotes_tag = posting.find('span', class_='js-ratings-positive-count')\n",
    "            upvotes = int(upvotes_tag.text.strip()) if upvotes_tag and upvotes_tag.text.isdigit() else 0\n",
    "\n",
    "            # Downvotes extrahieren\n",
    "            downvotes_tag = posting.find('span', class_='js-ratings-negative-count')\n",
    "            downvotes = int(downvotes_tag.text.strip()) if downvotes_tag and downvotes_tag.text.isdigit() else 0\n",
    "\n",
    "            # Anzahl der Follower des Nutzers extrahieren\n",
    "            user_followers_tag = posting.find('span', class_='upost-follower')\n",
    "            user_followers = int(user_followers_tag.text.strip()) if user_followers_tag and user_followers_tag.text.isdigit() else 0\n",
    "\n",
    "            comment_data = {\n",
    "                'commentID': commentID,\n",
    "                'author': username,\n",
    "                'user_followers': user_followers,\n",
    "                'datetime': datetime_obj,\n",
    "                'content': f\"{comment_header}\\n{comment_text}\".strip(),\n",
    "                'upvotes': upvotes,\n",
    "                'downvotes': downvotes,\n",
    "                'reply_on_comment': reply_on_comment,\n",
    "                'replies': []\n",
    "            }\n",
    "\n",
    "            # In die Map einfügen\n",
    "            comment_map[commentID] = comment_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Verarbeiten eines Kommentars: {e}\", exc_info=True)\n",
    "            continue \n",
    "\n",
    "    # Jetzt die verschachtelte Struktur aufbauen\n",
    "    for comment in comment_map.values():\n",
    "        parent_id = comment['reply_on_comment']\n",
    "        if parent_id and parent_id in comment_map:\n",
    "            parent_comment = comment_map[parent_id]\n",
    "            parent_comment['replies'].append(comment)\n",
    "        else:\n",
    "            comments_data.append(comment)\n",
    "\n",
    "    return comments_data, False  \n",
    "\n",
    "\n",
    "def scraping_fail(url, exception_message, logger):\n",
    "    current_date = datetime.datetime.now()\n",
    "    derStandard_collection.update_one(\n",
    "        {'scraping_info.url': url},\n",
    "        {\n",
    "            '$set': {\n",
    "                'scraping_info.status': 'failed',\n",
    "                'scraping_info.download_datetime': current_date\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    logger.warning(f\"Artikel übersprungen (fehlende Daten): {url}\")\n",
    "\n",
    "\n",
    "\n",
    "def scrape_articles(logger):\n",
    "    logger.info(\"Starte den Artikel-Scraping-Prozess.\")\n",
    "    driver = configure_driver(headless=True)\n",
    "\n",
    "    try:\n",
    "        # Alle URLs aus der 'derStandard' Collection holen, die noch nicht gescraped wurden\n",
    "        urls_to_scrape = list(derStandard_collection.find({'scraping_info.status': ''}, {'scraping_info.url': 1}))\n",
    "        logger.debug(f\"Anzahl der zu scrapenden URLs: {len(urls_to_scrape)}\")\n",
    "\n",
    "        # FRONTPAGE_URL mit heutigem Datum erstellen\n",
    "        frontpage_url = FRONTPAGE_URL + datetime.date.today().strftime(\"%Y/%m/%d\")\n",
    "        logger.info(f\"Navigiere zur Frontpage: {frontpage_url}\")\n",
    "\n",
    "        # Zur Frontpage navigieren, um das Pop-up zu schließen\n",
    "        driver.get(frontpage_url)\n",
    "        logger.debug(\"Frontpage geladen. Warte auf Pop-up.\")\n",
    "        time.sleep(5)  # Kurze Pause, um sicherzustellen, dass alles geladen ist\n",
    "\n",
    "        # Für jede URL in der Liste\n",
    "        for url_dict in urls_to_scrape:\n",
    "            full_url = url_dict['scraping_info']['url']\n",
    "            logger.info(f\"Verarbeite URL: {full_url}\")\n",
    "\n",
    "            # Überspringen spezifischer URLs, falls notwendig\n",
    "            if full_url.startswith(\"https://www.derstandard.at/story/3000000240211/w\"):\n",
    "                continue\n",
    "\n",
    "            # Seite laden mit Timeout von 10 Sekunden\n",
    "            driver.set_page_load_timeout(10)\n",
    "            try:\n",
    "                driver.get(full_url)\n",
    "            except TimeoutException:\n",
    "                scraping_fail(url=full_url, exception_message='Timeout nach 10 Sekunden', logger=logger)\n",
    "                continue\n",
    "\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            time.sleep(5)\n",
    "\n",
    "            try:\n",
    "                # Warten, bis die Seite vollständig geladen ist\n",
    "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "                logger.debug(f\"Seite {full_url} vollständig geladen.\")\n",
    "\n",
    "                # BeautifulSoup zum Parsen (für Elemente außerhalb des Shadow DOM)\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                logger.debug(f\"HTML-Inhalt von {full_url} mit BeautifulSoup geparst.\")\n",
    "\n",
    "                # manchmal ist die Seite anders strukturiert\n",
    "                old_design = soup.find(\"div\", class_=\"forum use-unobtrusive-ajax visible\")\n",
    "\n",
    "                # Rubrik/Kicker\n",
    "                kicker_tag = soup.find('h2', class_='article-kicker')\n",
    "                kicker = kicker_tag.get_text(strip=True) if kicker_tag else None\n",
    "\n",
    "                # Titel\n",
    "                title_tag = soup.find('h1', class_='article-title')\n",
    "                title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "                # Subtitel\n",
    "                subtitle_tag = soup.find('p', class_='article-subtitle')\n",
    "                subtitle = subtitle_tag.get_text(strip=True) if subtitle_tag else None\n",
    "\n",
    "                # Artikel-Byline (kann verschachtelt sein)\n",
    "                article_byline = get_article_byline(soup, logger)\n",
    "\n",
    "                # Datum und Uhrzeit extrahieren und in DATETIME konvertieren\n",
    "                article_datetime = get_article_datetime(soup, logger)\n",
    "\n",
    "                if article_datetime is None or title is None:\n",
    "                    scraping_fail(url=full_url, exception_message='Fehlendes Datum oder Titel', logger=logger)\n",
    "                    continue\n",
    "\n",
    "                # Anzahl der Postings extrahieren\n",
    "                posting_count = get_posting_count(soup, full_url, logger)\n",
    "\n",
    "                # Reaktionen extrahieren\n",
    "                reactions, reactions_warning = extract_reactions(driver, logger)\n",
    "\n",
    "                # Artikelinhalt extrahieren\n",
    "                paragraph_texts = get_paragraph_texts(soup, full_url, logger)\n",
    "\n",
    "                # Kommentare extrahieren\n",
    "                if old_design:\n",
    "                    forum_comments, comments_warning = extract_forum_comments_alternative(driver, logger)\n",
    "                else:\n",
    "                    forum_comments, comments_warning = extract_forum_comments_normal(driver, logger)\n",
    "\n",
    "                # Status bestimmen\n",
    "                if reactions_warning or comments_warning:\n",
    "                    status = 'warning'\n",
    "                else:\n",
    "                    status = 'success'\n",
    "\n",
    "                # Daten vorbereiten gemäß neuer Struktur\n",
    "                article_data = {\n",
    "                    'article.title': title,\n",
    "                    'article.subtitle': subtitle,\n",
    "                    'article.kicker': kicker,\n",
    "                    'article.text': paragraph_texts,\n",
    "                    'article.author': article_byline,\n",
    "                    'article.pubdate': article_datetime,\n",
    "                    'article.comments': forum_comments,\n",
    "                    'features.posting_count': posting_count,\n",
    "                    'features.reactions': reactions,\n",
    "                    'scraping_info.status': status,\n",
    "                    'scraping_info.download_datetime': datetime.datetime.now()\n",
    "                }\n",
    "\n",
    "                # Daten in die 'derStandard' Collection einfügen\n",
    "                derStandard_collection.update_one(\n",
    "                    {'scraping_info.url': full_url},\n",
    "                    {'$set': article_data}\n",
    "                )\n",
    "\n",
    "                logger.info(f\"Erfolgreich gescraped mit Status '{status}': {full_url} am {article_datetime}\")\n",
    "\n",
    "            except TimeoutException:\n",
    "                scraping_fail(url=full_url, exception_message='Timeout nach 10 Sekunden', logger=logger)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                exception_message = str(e)\n",
    "                scraping_fail(url=full_url, exception_message=exception_message, logger=logger)\n",
    "                logger.error(f\"Fehler beim Verarbeiten von {full_url}: {e}\", exc_info=True)\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Unerwarteter Fehler im Scraping-Prozess: {e}\", exc_info=True)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        logger.info(\"Browser erfolgreich geschlossen.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 22:22:49,198 - __main__ - INFO - Starte den Artikel-Scraping-Prozess.\n",
      "2024-11-11 22:23:05,467 - __main__ - INFO - Navigiere zur Frontpage: https://www.derstandard.at/2024/11/11\n",
      "2024-11-11 22:23:10,479 - __main__ - INFO - Verarbeite URL: https://www.derstandard.at/story/3000000240211/wie-gerecht-ist-unser-zugang-zur-gesundheitsversorgung-wirklich\n",
      "2024-11-11 22:23:10,484 - __main__ - INFO - Verarbeite URL: https://www.derstandard.at/story/2000142048886/unternehmer-reinhold-wuerth-ist-kaeufer-von-beckmann-rekordgemaelde\n",
      "2024-11-11 22:23:22,780 - __main__ - INFO - Erfolgreich gescraped mit Status 'success': https://www.derstandard.at/story/2000142048886/unternehmer-reinhold-wuerth-ist-kaeufer-von-beckmann-rekordgemaelde am 2022-12-22 17:23:00\n",
      "2024-11-11 22:23:22,783 - __main__ - INFO - Verarbeite URL: https://www.derstandard.at/story/2000142048685/eugh-entscheidet-gegen-schadenersatz-bei-krankheit-wegen-luftverschmutzung\n",
      "2024-11-11 22:23:30,356 - __main__ - INFO - Erfolgreich gescraped mit Status 'success': https://www.derstandard.at/story/2000142048685/eugh-entscheidet-gegen-schadenersatz-bei-krankheit-wegen-luftverschmutzung am 2022-12-22 17:22:00\n",
      "2024-11-11 22:23:30,359 - __main__ - INFO - Verarbeite URL: https://www.derstandard.at/story/2000142047473/orf-general-warnt-vor-anschein-der-wahlwerbung-sisimania\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "def main():\n",
    "    logger = setup_logger()\n",
    "    scrape_articles(logger)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "⚠️ **Diese Zelle ist deaktiviert und wurde nicht ausgeführt.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%deactivate\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException,\n",
    "    WebDriverException\n",
    ")\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import dateparser\n",
    "import re\n",
    "from dateparser import parse as dateparser_parse\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# global vars for local system\n",
    "CHROMEDRIVER_PATH = r\"chromedriver.exe\"\n",
    "FRONTPAGE_URL = \"https://www.derstandard.at/frontpage/\"\n",
    "\n",
    "\n",
    "def configure_driver(headless=True):\n",
    "    chrome_options = wd.ChromeOptions()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\")\n",
    "    chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_prefs = {\n",
    "        \"profile.default_content_settings.images\": 2,  # Bilder deaktivieren\n",
    "        \"profile.managed_default_content_settings.images\": 2\n",
    "    }\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_options.page_load_strategy = 'none'  # Seite wird nicht vollständig geladen, Timeout auf Seite selbst kontrollieren \n",
    "\n",
    "    service = ChromeService(executable_path=CHROMEDRIVER_PATH)\n",
    "    driver = wd.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # POPUP WEGKLICKEN\n",
    "    driver.get(FRONTPAGE_URL + datetime.date.today().strftime(\"%Y/%m/%d\"))\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.execute_script(\"return document.readyState\") == 'complete'\n",
    "        )\n",
    "        driver.switch_to.frame(driver.find_element(By.XPATH, \"/html/body/div/iframe\"))\n",
    "        driver.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div[3]/div[1]/button\").click()\n",
    "        driver.switch_to.parent_frame()\n",
    "    except NoSuchElementException:\n",
    "        raise Exception(\"popup nicht gefunden\")\n",
    "    \n",
    "    return driver\n",
    "\n",
    "\n",
    "\n",
    "# Logging konfigurieren\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)  # Setzen Sie das minimale Log-Level\n",
    "\n",
    "# Log-Format definieren\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Console-Handler hinzufügen\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)  # Konsole zeigt INFO und höhere Level\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# File-Handler hinzufügen\n",
    "file_handler = logging.FileHandler('scraper.log')\n",
    "file_handler.setLevel(logging.DEBUG)  # Datei speichert alle Log-Level\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "\n",
    "def extract_reactions(driver):\n",
    "    \"\"\"\n",
    "    Extrahiert die Reaktionen aus dem Shadow DOM der aktuellen Seite.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shadow_host = driver.find_element(By.CSS_SELECTOR, \"dst-community-reactions\")\n",
    "        shadow_root = expand_shadow_element(driver, shadow_host)\n",
    "        reactions_buttons = shadow_root.find_elements(By.CSS_SELECTOR, \"aside.reactions div.reactions--buttons button\")\n",
    "        reactions = {}\n",
    "        for button in reactions_buttons:\n",
    "            try:\n",
    "                count_element = button.find_element(By.TAG_NAME, \"strong\")\n",
    "                count = int(count_element.text.strip())\n",
    "            except (NoSuchElementException, ValueError):\n",
    "                count = 0\n",
    "            try:\n",
    "                sr_only = button.find_element(By.CSS_SELECTOR, \"span.sr-only\")\n",
    "                reaction_name = sr_only.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                reaction_name = button.text.replace(str(count), '').strip()\n",
    "            reactions[reaction_name] = count\n",
    "        return reactions\n",
    "    except NoSuchElementException:\n",
    "        logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Reaktionen konnten nicht extrahiert werden.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{inspect.currentframe().f_back.f_code.co_name} Fehler beim Extrahieren der Reaktionen: {e}\", exc_info=True)\n",
    "\n",
    "def parse_posting(posting_element):\n",
    "    # Parst ein einzelnes <dst-posting>-Element und extrahiert die relevanten Daten.\n",
    "    try:\n",
    "        # Extrahiere den Autor\n",
    "        author = \"Unbekannter Benutzer\"\n",
    "        try:\n",
    "            usermenu = posting_element.find_element(By.CSS_SELECTOR, \"dst-posting--user button\")\n",
    "            spans = usermenu.find_elements(By.CSS_SELECTOR, \"span > span\")\n",
    "            if spans:\n",
    "                author = spans[0].text.strip()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Extrahiere die Anzahl der Follower\n",
    "        user_followers = 0\n",
    "        try:\n",
    "            followers_div = posting_element.find_element(By.CSS_SELECTOR, \"dst-posting--user button div[title]\")\n",
    "            followers_text = followers_div.get_attribute(\"title\")\n",
    "            followers_match = re.search(r'\\d+', followers_text)\n",
    "            if followers_match:\n",
    "                user_followers = int(followers_match.group())\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        except ValueError:\n",
    "            logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Ungültige Follower-Zahl: '{followers_text}'.\")\n",
    "\n",
    "        # Extrahiere das Datum und die Uhrzeit\n",
    "        datetime_obj = None\n",
    "        try:\n",
    "            time_tag = posting_element.find_element(By.CSS_SELECTOR, \"time[data-date]\")\n",
    "            datetime_str = time_tag.get_attribute(\"data-date\")\n",
    "            datetime_obj = dateparser.parse(datetime_str, languages=['de'])\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Extrahiere den Inhalt des Postings\n",
    "        content = \"\"\n",
    "        try:\n",
    "            content_div = posting_element.find_element(By.CSS_SELECTOR, \"div.posting--content\")\n",
    "            headers = content_div.find_elements(By.TAG_NAME, \"h1\")\n",
    "            paragraphs = content_div.find_elements(By.TAG_NAME, \"p\")\n",
    "            header_text = \"\\n\".join([h.text for h in headers]) if headers else \"\"\n",
    "            paragraph_text = \"\\n\".join([p.text for p in paragraphs]) if paragraphs else \"\"\n",
    "            content = \"\\n\".join([header_text, paragraph_text]).strip()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Extrahiere Upvotes und Downvotes\n",
    "        upvotes = 0\n",
    "        downvotes = 0\n",
    "        try:\n",
    "            ratinglog = posting_element.find_element(By.CSS_SELECTOR, \"dst-posting--ratinglog\")\n",
    "            positiveratings = ratinglog.get_attribute(\"positiveratings\")\n",
    "            negativeratings = ratinglog.get_attribute(\"negativeratings\")\n",
    "            upvotes = int(positiveratings) if positiveratings and positiveratings.isdigit() else 0\n",
    "            downvotes = int(negativeratings) if negativeratings and negativeratings.isdigit() else 0\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        except ValueError:\n",
    "            logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Ungültige Upvote/Downvote-Zahlen gefunden.\")\n",
    "\n",
    "        # Extrahiere Parent-Kommentar-ID (falls Antwort)\n",
    "        parent_id = posting_element.get_attribute(\"data-parentpostingid\")\n",
    "        reply_on_comment = int(parent_id) if parent_id and parent_id.isdigit() else None\n",
    "\n",
    "        # Extrahiere Kommentar-ID\n",
    "        commentID = posting_element.get_attribute(\"data-postingid\")\n",
    "        commentID = int(commentID) if commentID and commentID.isdigit() else None\n",
    "\n",
    "        # Erstelle das Kommentar-Dictionary\n",
    "        comment = {\n",
    "            'commentID': commentID,\n",
    "            'author': author,\n",
    "            'user_followers': user_followers,\n",
    "            'datetime': datetime_obj,\n",
    "            'content': content,\n",
    "            'upvotes': upvotes,\n",
    "            'downvotes': downvotes,\n",
    "            'reply_on_comment': reply_on_comment,\n",
    "            'replies': []\n",
    "        }\n",
    "\n",
    "        return comment\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{inspect.currentframe().f_back.f_code.co_name} Fehler beim Parsen eines Postings: {e}\", exc_info=True)\n",
    "        return None\n",
    "    \n",
    "def extract_forum_comments_normal(driver, max_comments=50):\n",
    "    \"\"\"\n",
    "    Extrahiert Benutzerkommentare aus dem Shadow DOM der aktuellen Seite und\n",
    "    bildet verschachtelte Antworten ab.\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    count = 0\n",
    "    try:\n",
    "        forum_host = driver.find_element(By.CSS_SELECTOR, \"dst-forum\")\n",
    "        forum_shadow = expand_shadow_element(driver, forum_host)\n",
    "        main_content = forum_shadow.find_element(By.CSS_SELECTOR, \"main.forum--main\")\n",
    "        children = main_content.find_elements(By.CSS_SELECTOR, \":scope > *\")\n",
    "        current_parent = None\n",
    "\n",
    "        for child in children:\n",
    "            if count >= max_comments:\n",
    "                break\n",
    "            tag_name = child.tag_name.lower()\n",
    "            if tag_name == \"dst-posting\":\n",
    "                comment = parse_posting(child)\n",
    "                if comment:\n",
    "                    comments.append(comment)\n",
    "                    current_parent = comment\n",
    "                    count += 1\n",
    "            elif tag_name == \"section\":\n",
    "                classes = child.get_attribute(\"class\")\n",
    "                if classes and \"thread\" in classes:\n",
    "                    if not current_parent:\n",
    "                        logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Thread-Sektion gefunden, aber kein aktueller Parent.\")\n",
    "                        continue\n",
    "                    reply_postings = child.find_elements(By.CSS_SELECTOR, \"dst-posting\")\n",
    "                    for reply in reply_postings:\n",
    "                        if count >= max_comments:\n",
    "                            break\n",
    "                        reply_comment = parse_posting(reply)\n",
    "                        if reply_comment:\n",
    "                            current_parent['replies'].append(reply_comment)\n",
    "                            count += 1\n",
    "        return comments\n",
    "    except NoSuchElementException:\n",
    "        logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Forum-Elemente nicht gefunden.\")\n",
    "        return comments\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{inspect.currentframe().f_back.f_code.co_name} Fehler beim Extrahieren der Forenkommentare: {e}\", exc_info=True)\n",
    "        return comments\n",
    "\n",
    "\n",
    "\n",
    "def extract_forum_comments_alternative(driver, max_comments=50):\n",
    "    comments_data = []\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    postings = soup.find_all('div', class_='posting', attrs={'data-postingid': True})\n",
    "\n",
    "    for posting in postings[:max_comments]:  # Bis zu 10 Kommentare sammeln\n",
    "        try:\n",
    "            commentID = posting.get('data-postingid')\n",
    "            username = posting.get('data-communityname') or 'gelöschtes Profil'\n",
    "            reply_on_comment = posting.get('data-parentpostingid')\n",
    "            reply_on_comment = int(reply_on_comment) if reply_on_comment else None\n",
    "\n",
    "            # Datum und Uhrzeit des Kommentars extrahieren\n",
    "            datetime_tag = posting.find('span', class_='js-timestamp')\n",
    "            if datetime_tag and datetime_tag.text:\n",
    "                datetime_str = datetime_tag.text.strip()\n",
    "                datetime_obj = dateparser.parse(datetime_str, languages=['de'])\n",
    "            else:\n",
    "                datetime_obj = None \n",
    "\n",
    "            # Kommentarüberschrift extrahieren\n",
    "            comment_header_tag = posting.find('h4', class_='upost-title')\n",
    "            comment_header = comment_header_tag.text.strip() if comment_header_tag else None\n",
    "\n",
    "            # Kommentartext extrahieren\n",
    "            comment_body = posting.find('div', class_='upost-text')\n",
    "            comment_text = comment_body.get_text(separator=' ', strip=True) if comment_body else None\n",
    "\n",
    "            # Upvotes extrahieren\n",
    "            upvotes_tag = posting.find('span', class_='js-ratings-positive-count')\n",
    "            upvotes = int(upvotes_tag.text.strip()) if upvotes_tag and upvotes_tag.text else 0\n",
    "\n",
    "            # Downvotes extrahieren\n",
    "            downvotes_tag = posting.find('span', class_='js-ratings-negative-count')\n",
    "            downvotes = int(downvotes_tag.text.strip()) if downvotes_tag and downvotes_tag.text else 0\n",
    "\n",
    "            # Anzahl der Follower des Nutzers extrahieren\n",
    "            user_followers_tag = posting.find('span', class_='upost-follower')\n",
    "            user_followers = int(user_followers_tag.text.strip()) if user_followers_tag and user_followers_tag.text else 0\n",
    "\n",
    "            comments_data.append({\n",
    "                'commentID': int(commentID),\n",
    "                'author': username,\n",
    "                'user_followers': user_followers,\n",
    "                'datetime': datetime_obj,\n",
    "                'content': comment_header+comment_text,\n",
    "                'upvotes': upvotes,\n",
    "                'downvotes': downvotes,\n",
    "                'replies': reply_on_comment\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{inspect.currentframe().f_back.f_code.co_name} Fehler beim Verarbeiten eines Kommentars in Artikel: {e}\", exc_info=True)\n",
    "            continue \n",
    "\n",
    "    return comments_data\n",
    "\n",
    "\n",
    "\n",
    "def parse_comment_datetime(datetime_str):\n",
    "    return dateparser.parse(datetime_str, languages=['de'])\n",
    "\n",
    "def extract_forum_comments_alternative(driver, max_comments=50):\n",
    "    \"\"\"\n",
    "    Extrahiert Benutzerkommentare aus der aktuellen Seite unter Verwendung von BeautifulSoup\n",
    "    und bildet verschachtelte Antworten ab.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    comment_map = {}\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    postings = soup.find_all('div', class_='posting', attrs={'data-postingid': True})\n",
    "\n",
    "    for posting in postings[:max_comments]:\n",
    "        try:\n",
    "            commentID = posting.get('data-postingid')\n",
    "            if not commentID or not commentID.isdigit():\n",
    "                continue\n",
    "            commentID = int(commentID)\n",
    "\n",
    "            username = posting.get('data-communityname') or 'gelöschtes Profil'\n",
    "\n",
    "            reply_on_comment = posting.get('data-parentpostingid')\n",
    "            reply_on_comment = int(reply_on_comment) if reply_on_comment and reply_on_comment.isdigit() else None\n",
    "\n",
    "            # Datum und Uhrzeit des Kommentars extrahieren\n",
    "            datetime_tag = posting.find('span', class_='js-timestamp')\n",
    "            if datetime_tag and datetime_tag.text:\n",
    "                datetime_str = datetime_tag.text.strip()\n",
    "                datetime_obj = parse_comment_datetime(datetime_str)\n",
    "            else:\n",
    "                datetime_obj = None \n",
    "\n",
    "            # Kommentarüberschrift extrahieren\n",
    "            comment_header_tag = posting.find('h4', class_='upost-title')\n",
    "            comment_header = comment_header_tag.text.strip() if comment_header_tag else \"\"\n",
    "\n",
    "            # Kommentartext extrahieren\n",
    "            comment_body = posting.find('div', class_='upost-text')\n",
    "            comment_text = comment_body.get_text(separator=' ', strip=True) if comment_body else \"\"\n",
    "\n",
    "            # Upvotes extrahieren\n",
    "            upvotes_tag = posting.find('span', class_='js-ratings-positive-count')\n",
    "            upvotes = int(upvotes_tag.text.strip()) if upvotes_tag and upvotes_tag.text.isdigit() else 0\n",
    "\n",
    "            # Downvotes extrahieren\n",
    "            downvotes_tag = posting.find('span', class_='js-ratings-negative-count')\n",
    "            downvotes = int(downvotes_tag.text.strip()) if downvotes_tag and downvotes_tag.text.isdigit() else 0\n",
    "\n",
    "            # Anzahl der Follower des Nutzers extrahieren\n",
    "            user_followers_tag = posting.find('span', class_='upost-follower')\n",
    "            user_followers = int(user_followers_tag.text.strip()) if user_followers_tag and user_followers_tag.text.isdigit() else 0\n",
    "\n",
    "            comment_data = {\n",
    "                'commentID': commentID,\n",
    "                'author': username,\n",
    "                'user_followers': user_followers,\n",
    "                'datetime': datetime_obj,\n",
    "                'content': f\"{comment_header}\\n{comment_text}\".strip(),\n",
    "                'upvotes': upvotes,\n",
    "                'downvotes': downvotes,\n",
    "                'reply_on_comment': reply_on_comment,\n",
    "                'replies': []\n",
    "            }\n",
    "\n",
    "            # In die Map einfügen\n",
    "            comment_map[commentID] = comment_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{inspect.currentframe().f_back.f_code.co_name} Fehler beim Verarbeiten eines Kommentars: {e}\", exc_info=True)\n",
    "            continue \n",
    "\n",
    "    # Jetzt die verschachtelte Struktur aufbauen\n",
    "    for comment in comment_map.values():\n",
    "        parent_id = comment['reply_on_comment']\n",
    "        if parent_id and parent_id in comment_map:\n",
    "            parent_comment = comment_map[parent_id]\n",
    "            parent_comment['replies'].append(comment)\n",
    "        else:\n",
    "            comments_data.append(comment)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_article_byline(soup):\n",
    "    article_byline = {}\n",
    "    article_byline_tag = soup.find('div', class_='article-byline')\n",
    "    if article_byline_tag:\n",
    "        # Storylabels extrahieren\n",
    "        storylabels_tag = article_byline_tag.find('div', class_='storylabels')\n",
    "        if storylabels_tag:\n",
    "            storylabels = storylabels_tag.get_text(strip=True)\n",
    "            article_byline['storylabels'] = storylabels\n",
    "\n",
    "        # Article origins extrahieren\n",
    "        article_origins_tag = article_byline_tag.find('div', class_='article-origins')\n",
    "        if article_origins_tag:\n",
    "            article_origins = article_origins_tag.get_text(strip=True)\n",
    "            article_byline['article_origins'] = article_origins\n",
    "        else:\n",
    "            # Fallback für einfachen Autorentext\n",
    "            author_simple = article_byline_tag.find('span', class_='simple')\n",
    "            if author_simple:\n",
    "                article_byline['article_origins'] = author_simple.get_text(strip=True)\n",
    "    else:\n",
    "        article_byline = None\n",
    "        logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Keine Artikel-Byline gefunden.\")\n",
    "    return article_byline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_article_datetime(soup):\n",
    "    time_tag = soup.find('time', class_='article-pubdate')\n",
    "    if time_tag:\n",
    "        if time_tag.has_attr('datetime'):\n",
    "            datetime_str = time_tag['datetime'].strip()\n",
    "            datetime_str = datetime_str.replace('\\n', '').strip()\n",
    "        else:\n",
    "            datetime_str = time_tag.get_text(strip=True)\n",
    "        try:\n",
    "            article_datetime = datetime.datetime.fromisoformat(datetime_str)\n",
    "        except ValueError:\n",
    "            datetime_text = time_tag.get_text(strip=True)\n",
    "            article_datetime = dateparser.parse(datetime_text, languages=['de'])\n",
    "    else:\n",
    "        article_datetime = None\n",
    "        logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Kein Datum gefunden.\")\n",
    "    return article_datetime\n",
    "\n",
    "\n",
    "\n",
    "def get_posting_count(soup, full_url):\n",
    "    posting_count = None\n",
    "    try:\n",
    "        posting_count_tag = soup.find('span', class_='js-forum-postingcount')\n",
    "        if posting_count_tag:\n",
    "            posting_count_text = posting_count_tag.contents[0].strip()\n",
    "            posting_count = int(posting_count_text)\n",
    "            return posting_count\n",
    "    except (AttributeError, ValueError):\n",
    "        posting_count = None\n",
    "    try:\n",
    "        community_section = soup.find('section', id='story-community')\n",
    "        header_div = community_section.find('div', class_='story-community-header')\n",
    "        h1_tag = header_div.find('h1')\n",
    "        h1_text = h1_tag.get_text(strip=True)\n",
    "        match = re.search(r'Forum:\\s*(\\d+)\\s*Postings', h1_text)\n",
    "        posting_count = int(match.group(1))\n",
    "        return posting_count\n",
    "    except (AttributeError, ValueError):\n",
    "        posting_count = None\n",
    "        logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Ungültige Posting-Anzahl in {full_url}\")\n",
    "    return posting_count\n",
    "\n",
    "                    \n",
    "\n",
    "def get_paragraph_texts(soup,full_url):\n",
    "    # Artikelinhalt extrahieren\n",
    "    paragraph_texts = None\n",
    "    try:\n",
    "        article_body = soup.find('div', class_='article-body')\n",
    "        if article_body:\n",
    "            # Alle 'href'-Attribute entfernen\n",
    "            for a_tag in article_body.find_all('a'):\n",
    "                del a_tag['href']\n",
    "            logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Alle 'href'-Attribute aus Artikeltext entfernt.\")\n",
    "\n",
    "            # Unerwünschte Elemente entfernen\n",
    "            for ad in article_body.find_all(['ad-container', 'ad-slot', 'ad']):\n",
    "                ad.decompose()\n",
    "            for figure in article_body.find_all('figure'):\n",
    "                figure.decompose()\n",
    "            for unwanted in article_body.find_all(['aside', 'nav', 'div'], attrs={'data-section-type': 'supplemental'}):\n",
    "                unwanted.decompose()\n",
    "            logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Unerwünschte Elemente aus Artikeltext entfernt.\")\n",
    "\n",
    "            # Paragraphen extrahieren und in Liste umwandeln\n",
    "            paragraphs = article_body.find_all('p')\n",
    "            paragraph_texts = [p.get_text(strip=True) for p in paragraphs]\n",
    "            logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Extrahierte Paragraphen: {len(paragraph_texts)} in {full_url}\")\n",
    "        else:\n",
    "            logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} Kein Artikelinhalt gefunden in {full_url}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{inspect.currentframe().f_back.f_code.co_name} Fehler beim Extrahieren des Artikelinhalts: {e}\", exc_info=True)\n",
    "    return paragraph_texts\n",
    "\n",
    "\n",
    "\n",
    "def scraping_fail(url,exception_message):\n",
    "    current_date = datetime.datetime.now()\n",
    "    failed_collection.insert_one({\n",
    "        'url': url,\n",
    "        'date': current_date,\n",
    "        'exception': exception_message\n",
    "    })\n",
    "    logger.warning(f\"{inspect.currentframe().f_back.f_code.co_name} Artikel übersprungen (fehlende Daten): {url}\")\n",
    "\n",
    "    urls_collection.update_one(\n",
    "        {'URL': url},\n",
    "        {'$set': {'download_date': current_date}}\n",
    "    )\n",
    "    logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} 'download_date' für {url} aktualisiert.\")\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def expand_shadow_element(driver, element):\n",
    "    \"\"\"Erweitert ein Shadow DOM-Element und gibt das Shadow Root zurück.\"\"\"\n",
    "    shadow_root = driver.execute_script('return arguments[0].shadowRoot', element)\n",
    "    return shadow_root\n",
    "\n",
    "def scrape_articles():\n",
    "    logger.info(\"Starte den Artikel-Scraping-Prozess.\")\n",
    "    driver = configure_driver()\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "\n",
    "    try:\n",
    "        # Alle URLs aus der 'urls' Sammlung holen, die noch nicht gescraped wurden\n",
    "        urls_to_scrape = list(urls_collection.find({'download_date': None}, {'URL': 1}))\n",
    "        logger.debug(f\"Anzahl der zu scrapenden URLs: {len(urls_to_scrape)}\")\n",
    "\n",
    "        # FRONTPAGE_URL mit heutigem Datum erstellen\n",
    "        FRONTPAGE_URL = \"https://www.derstandard.at/\" + datetime.date.today().strftime(\"%Y/%m/%d\")\n",
    "        logger.info(f\"Navigiere zur Frontpage: {FRONTPAGE_URL}\")\n",
    "\n",
    "        # Zur Frontpage navigieren, um das Pop-up zu schließen\n",
    "        driver.get(FRONTPAGE_URL)\n",
    "        logger.debug(\"Frontpage geladen. Warte auf Pop-up.\")\n",
    "        time.sleep(5)  # Kurze Pause, um sicherzustellen, dass alles geladen ist\n",
    "        # Für jede URL in der Liste\n",
    "        for url_dict in urls_to_scrape:\n",
    "            full_url = url_dict['URL']\n",
    "            #full_url = url_dict\n",
    "            logger.info(f\"Verarbeite URL: {full_url}\")\n",
    "\n",
    "            # Seite laden mit Timeout von 10 Sekunden\n",
    "            driver.set_page_load_timeout(10)\n",
    "            driver.get(full_url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            try:\n",
    "                # Warten, bis die Seite vollständig geladen ist\n",
    "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "                logger.debug(f\"Seite {full_url} vollständig geladen.\")\n",
    "\n",
    "                # BeautifulSoup zum Parsen (für Elemente außerhalb des Shadow DOM)\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                logger.debug(f\"HTML-Inhalt von {full_url} mit BeautifulSoup geparst.\")\n",
    "\n",
    "                # manchmal ist die seite anders strukturiert\n",
    "                old_design = soup.find(\"div\", class_=\"forum use-unobtrusive-ajax visible\")\n",
    "                \n",
    "                # Artikelklasse speichern (z.B. 'story-article', 'video-article')\n",
    "                article_element = soup.find('article')\n",
    "                article_classes = article_element.get('class', []) if article_element else []\n",
    "                article_class = ' '.join(article_classes) if article_classes else None\n",
    "\n",
    "                # Rubrik/Kicker\n",
    "                kicker_tag = soup.find('h2', class_='article-kicker')\n",
    "                kicker = kicker_tag.get_text(strip=True) if kicker_tag else None\n",
    "\n",
    "                # Titel\n",
    "                title_tag = soup.find('h1', class_='article-title')\n",
    "                title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "                # Subtitel\n",
    "                subtitle_tag = soup.find('p', class_='article-subtitle')\n",
    "                subtitle = subtitle_tag.get_text(strip=True) if subtitle_tag else None\n",
    "\n",
    "                # Artikel-Byline (kann verschachtelt sein)\n",
    "                article_byline = get_article_byline(soup)\n",
    "\n",
    "                # Datum und Uhrzeit extrahieren und in DATETIME konvertieren\n",
    "                article_datetime = get_article_datetime(soup)\n",
    "\n",
    "\n",
    "                if article_datetime is None or title is None:\n",
    "                    scraping_fail(url=full_url, exception_message='Fehlendes Datum oder Titel')\n",
    "                    continue\n",
    "\n",
    "                # Anzahl der Postings extrahieren (js-forum-postingcount)\n",
    "                posting_count = get_posting_count(soup, full_url)\n",
    "\n",
    "                # Reaktionen extrahieren\n",
    "                reactions = extract_reactions(driver)\n",
    "\n",
    "                # Artikelinhalt extrahieren\n",
    "                paragraph_texts = get_paragraph_texts(soup, full_url)\n",
    "\n",
    "                # Kommentare extrahieren\n",
    "                if old_design:\n",
    "                    forum_comments = extract_forum_comments_alternative(driver)\n",
    "                else:\n",
    "                    forum_comments = extract_forum_comments_normal(driver)\n",
    "\n",
    "\n",
    "                # Daten vorbereiten\n",
    "                article_data = {\n",
    "                    'url': full_url,\n",
    "                    'article_class': article_class,\n",
    "                    'kicker': kicker,\n",
    "                    'title': title,\n",
    "                    'subtitle': subtitle,\n",
    "                    'article_byline': article_byline,\n",
    "                    'datetime': article_datetime,\n",
    "                    'posting_count': posting_count,\n",
    "                    'reactions': reactions,\n",
    "                    'article_text': paragraph_texts,\n",
    "                    'forum_comments': forum_comments\n",
    "                }\n",
    "\n",
    "                # Daten in die MongoDB einfügen\n",
    "                result = articles_collection.update_one(\n",
    "                    {'url': full_url},\n",
    "                    {'$setOnInsert': article_data},\n",
    "                    upsert=True\n",
    "                )\n",
    "\n",
    "                if result.upserted_id is None:\n",
    "                    logger.info(f\"Artikel bereits vorhanden (Duplikat): {full_url}\")\n",
    "                else:\n",
    "                    logger.info(f\"Erfolgreich gescraped: {full_url} am {article_datetime}\")\n",
    "\n",
    "                # 'download_date' in der 'urls' Sammlung aktualisieren\n",
    "                current_date = datetime.datetime.now()\n",
    "                urls_collection.update_one(\n",
    "                    {'URL': full_url},\n",
    "                    {'$set': {'download_date': current_date}}\n",
    "                )\n",
    "                logger.debug(f\"'download_date' für {full_url} aktualisiert.\")\n",
    "\n",
    "            except TimeoutException:\n",
    "                current_date = datetime.datetime.now()\n",
    "                exception_message = 'Timeout nach 10 Sekunden'\n",
    "                failed_collection.insert_one({\n",
    "                    'url': full_url,\n",
    "                    'date': current_date,\n",
    "                    'exception': exception_message\n",
    "                })\n",
    "                logger.error(f\"{inspect.currentframe().f_back.f_code.co_name} Timeout beim Verarbeiten von {full_url}\")\n",
    "\n",
    "                urls_collection.update_one(\n",
    "                    {'URL': full_url},\n",
    "                    {'$set': {'download_date': current_date}}\n",
    "                )\n",
    "                logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} 'download_date' für {full_url} nach Timeout aktualisiert.\")\n",
    "                continue\n",
    "\n",
    "            except Exception as e:\n",
    "                current_date = datetime.datetime.now()\n",
    "                exception_message = str(e)\n",
    "                failed_collection.insert_one({\n",
    "                    'url': full_url,\n",
    "                    'date': current_date,\n",
    "                    'exception': exception_message\n",
    "                })\n",
    "                logger.error(f\"Fehler beim Verarbeiten von {full_url}: {e}\", exc_info=True)\n",
    "\n",
    "                urls_collection.update_one(\n",
    "                    {'URL': full_url},\n",
    "                    {'$set': {'download_date': current_date}}\n",
    "                )\n",
    "                logger.debug(f\"{inspect.currentframe().f_back.f_code.co_name} 'download_date' für {full_url} nach Fehler aktualisiert.\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"{inspect.currentframe().f_back.f_code.co_name} Unerwarteter Fehler im Scraping-Prozess: {e}\", exc_info=True)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        logger.info(f\"{inspect.currentframe().f_back.f_code.co_name} Browser erfolgreich geschlossen.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_articles()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
